# -*- coding: utf-8 -*-
"""Anamolies in Financial Transaction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QH-SdFbcTX1t5ctetFvF5P0O5vS9JJMM
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import IsolationForest
from sklearn.cluster import DBSCAN
from pyod.models.iforest import IForest
from pyod.utils.data import evaluate_print

# Load the dataset
# Replace 'dataset.csv' with the path to your actual dataset
file_path = 'transaction_anomalies_dataset.csv'
data = pd.read_csv(file_path)

# Preview the data
print(data.head())
print(data.info())

# 1. Preprocessing
# Drop irrelevant columns (e.g., Transaction_ID)
if 'Transaction_ID' in data.columns:
    data = data.drop(columns=['Transaction_ID'])

# Handle missing values
# Handle missing values
# Only calculate median for numeric columns
numeric_data = data.select_dtypes(include=np.number)
data[numeric_data.columns] = numeric_data.fillna(numeric_data.median())

# Alternatively, you can specify a method for non-numeric columns:
# data = data.fillna(data.median(numeric_only=True)) #for pandas 2.0 and above
# data = data.fillna(method='ffill') # forward fill for non-numeric

# Encode categorical variables
label_encoder = LabelEncoder()
if 'Gender' in data.columns:
    data['Gender'] = label_encoder.fit_transform(data['Gender'])

if 'Account_Type' in data.columns:
    data['Account_Type'] = label_encoder.fit_transform(data['Account_Type'])

# Normalize numerical columns
scaler = StandardScaler()
numerical_columns = [
    'Transaction_Amount',
    'Transaction_Volume',
    'Average_Transaction_Amount',
    'Frequency_of_Transactions',
    'Time_Since_Last_Transaction',
    'Income',
    'Age',
]
data[numerical_columns] = scaler.fit_transform(data[numerical_columns])

# 2. Exploratory Data Analysis (EDA)
# Visualize distribution of key variables
plt.figure(figsize=(10, 6))
sns.boxplot(data=data[numerical_columns])
plt.title('Boxplot of Numerical Features')
plt.xticks(rotation=45)
plt.show()

# Pairplot for trends
sns.pairplot(data, hue='Gender')
plt.show()

# 3. Anomaly Detection
# Isolation Forest
isolation_forest = IsolationForest(n_estimators=100, contamination=0.05, random_state=42)
data['Anomaly_IF'] = isolation_forest.fit_predict(data[numerical_columns])

# DBSCAN (optional)
dbscan = DBSCAN(eps=1.5, min_samples=5)
data['Cluster_DBSCAN'] = dbscan.fit_predict(data[numerical_columns])

# PyOD Isolation Forest for Anomaly Scoring
clf = IForest(contamination=0.05)
clf.fit(data[numerical_columns])
data['Anomaly_Score'] = clf.decision_scores_

# 4. Results
# Isolation Forest Anomalies
anomalies_if = data[data['Anomaly_IF'] == -1]
print(f"Isolation Forest detected {len(anomalies_if)} anomalies.")

# Visualization of anomalies
plt.figure(figsize=(10, 6))
sns.scatterplot(x=data['Transaction_Amount'], y=data['Transaction_Volume'], hue=data['Anomaly_IF'])
plt.title('Isolation Forest Anomalies')
plt.show()

# Save the results to a new CSV file
data.to_csv('anomaly_detection_results.csv', index=False)
print("Anomaly detection results saved to 'anomaly_detection_results.csv'.")

import pandas as pd
import numpy as np
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler, LabelEncoder

# Load the dataset
# Replace 'dataset.csv' with the path to your actual dataset
file_path = 'transaction_anomalies_dataset.csv'
data = pd.read_csv(file_path)

# Preprocessing: Drop irrelevant columns (e.g., Transaction_ID)
if 'Transaction_ID' in data.columns:
    data = data.drop(columns=['Transaction_ID'])

# Handle missing values
numeric_data = data.select_dtypes(include=np.number)
data[numeric_data.columns] = numeric_data.fillna(numeric_data.median())

# Encode categorical variables
label_encoder = LabelEncoder()
if 'Gender' in data.columns:
    data['Gender'] = label_encoder.fit_transform(data['Gender'])

if 'Account_Type' in data.columns:
    data['Account_Type'] = label_encoder.fit_transform(data['Account_Type'])

# Normalize numerical columns
scaler = StandardScaler()
numerical_columns = [
    'Transaction_Amount',
    'Transaction_Volume',
    'Average_Transaction_Amount',
    'Frequency_of_Transactions',
    'Time_Since_Last_Transaction',
    'Income',
    'Age',
]
data[numerical_columns] = scaler.fit_transform(data[numerical_columns])

# Anomaly Detection using Isolation Forest
isolation_forest = IsolationForest(n_estimators=100, contamination=0.05, random_state=42)
data['Anomaly_IF'] = isolation_forest.fit_predict(data[numerical_columns])

# Map the results to 'Anomaly' or 'Not Anomaly'
data['Anomaly'] = data['Anomaly_IF'].map({1: 'Not Anomaly', -1: 'Anomaly'})

# Drop intermediate column (optional)
data = data.drop(columns=['Anomaly_IF'])

# Save the updated dataset to a new CSV file
data.to_csv('anomaly_detection_with_labels.csv', index=False)
print("Anomaly detection results saved to 'anomaly_detection_with_labels.csv'.")

pip install pyod